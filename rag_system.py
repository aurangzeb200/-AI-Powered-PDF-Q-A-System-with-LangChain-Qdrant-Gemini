# -*- coding: utf-8 -*-
"""RAG_System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OZiZ_bbrkGoCTxHQnxFX4KNwQ0jAPYYe
"""

import torch
import os
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from qdrant_client import QdrantClient, models
from langchain_qdrant import FastEmbedSparse
from pypdf import PdfReader
from google import genai
from google.colab import userdata

# Device setup
device = "cuda" if torch.cuda.is_available() else "cpu"
print("✅ Using device:", device)

# File loading
filename = "Assembly Lang Programming by Sir Belal Hashmi.pdf"
filepath = f"/content/{filename}"
if not os.path.exists(filepath):
    raise FileNotFoundError(f"File not found: {filepath}")

try:
    loader = PyPDFLoader(filepath)
    docs = loader.load()
    print("✅ Loaded", len(docs), "document(s)")
except Exception as e:
    raise ValueError(f"Failed to load {filename}: {e}")

# Extract PDF metadata
reader = PdfReader(filepath)
pdf_metadata = reader.metadata
print("PDF Metadata:", pdf_metadata)

# Chunking
splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=50)
chunks = splitter.split_documents(docs)
print("✅ Split into", len(chunks), "chunks")
print("Sample metadata:", chunks[0].metadata)

# Embedding models
embedding_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-mpnet-base-v2",
    model_kwargs={"device": device},
    encode_kwargs={"normalize_embeddings": True}
)
sparse_embedding_model = FastEmbedSparse(model_name="Qdrant/bm25")

# Qdrant setup
client = QdrantClient(":memory:")  # Or path="/tmp/qdrant_db" for persistent storage
collection_name = "pdf_chunks"

try:
    if client.collection_exists(collection_name):
        client.delete_collection(collection_name)
        print("✅ Existing collection 'pdf_chunks' deleted")
    client.create_collection(
        collection_name=collection_name,
        vectors_config={
            "dense": models.VectorParams(size=768, distance=models.Distance.COSINE)
        },
        sparse_vectors_config={
            "sparse": models.SparseVectorParams(
                index=models.SparseIndexParams(on_disk=False)
            )
        }
    )
    print("✅ Collection 'pdf_chunks' created successfully")
except Exception as e:
    print(f"Collection creation failed: {e}")

# Embed and upsert
chunk_texts = [chunk.page_content for chunk in chunks]
dense_embeddings = embedding_model.embed_documents(chunk_texts)
sparse_embeddings = sparse_embedding_model.embed_documents(chunk_texts)

payloads = [
    {
        "text": chunk.page_content,
        "page": int(chunk.metadata.get("page_label", 0)),
        "source": filename,
        "embedding_model": "sentence-transformers/all-mpnet-base-v2"
    }
    for chunk in chunks
]

try:
    points = [
        models.PointStruct(
            id=i,
            vector={
                "dense": dense_vec,
                "sparse": models.SparseVector(indices=sparse_vec.indices, values=sparse_vec.values)
            },
            payload=payload
        )
        for i, (dense_vec, sparse_vec, payload) in enumerate(zip(dense_embeddings, sparse_embeddings, payloads))
    ]
    client.upsert(collection_name=collection_name, points=points)
    print("✅ Upserted", len(chunks), "points")
except Exception as e:
    print(f"Upsert failed: {e}")

# Index payloads for faster filtering
try:
    client.create_payload_index(collection_name=collection_name, field_name="page", field_schema="integer")
    client.create_payload_index(collection_name=collection_name, field_name="source", field_schema="keyword")
    print("✅ Payload indexes created")
except Exception as e:
    print(f"Payload indexing failed: {e}")

# Gemini setup
try:
    gem_client = genai.Client(api_key=userdata.get('GOOGLE_API_KEY'))
except Exception as e:
    print(f"Gemini API initialization failed: {e}")
    exit()

pp = """
Answer the question using the context and metadata below. If the answer isn’t clear, say "I don’t know exactly" and provide a clue based on the source or context.

Follow these exact rules:

- Begin with a clear, real definition in one line. No vague phrases like “low-level” without explanation.
- Focus on what the concept does and how it is used in code.
- Explain any technical word (like CPU, machine code) in one short line right after.
- Use bullet points only for listing commands or logical steps.
- Do not explain basic English words like "programmer" or "instruction."
- If the concept relates to control flow or decision-making (e.g., "jump"), clearly state *when* and *why* it is used in code.
- Do not add metaphors or examples unless asked.
- Keep the explanation under 6 lines.



[Term] is [definition].

[1–2 lines about how it works in code.]

- [Command or keyword] → [brief meaning]

[Technical word] = [short explanation]

Context:
{context}

Metadata:
Source: {source}
Author (if available): {author}

Question: {query}
"""

# Interactive query loop
while True:
    query = input("\nYour question (or 'exit'): ")
    if query.lower() == "exit":
        break

    try:
        dense_query = embedding_model.embed_query(query)
        sparse_query = sparse_embedding_model.embed_query(query)
        results = client.query_points(
            collection_name=collection_name,
            prefetch=[
                models.Prefetch(query=dense_query, using="dense", limit=20),
                models.Prefetch(
                    query=models.SparseVector(indices=sparse_query.indices, values=sparse_query.values),
                    using="sparse",
                    limit=20
                )
            ],
            query=models.FusionQuery(fusion=models.Fusion.RRF),
            query_filter=models.Filter(must=[models.FieldCondition(key="source", match=models.MatchValue(value=filename))]),
            limit=3,
            with_payload=True
        ).points

        context = "\n\n".join([doc.payload["text"] for doc in results])
        source = results[0].payload["source"] if results else filename
        author = pdf_metadata.get("/Author", "Unknown")
        prompt = pp.format(context=context, source=source, author=author, query=query)

        response = gem_client.models.generate_content(
            model="gemini-2.5-flash",
            contents=prompt
        )
        print("\nGemini Answer:", response.text)
    except Exception as e:
        print(f"Query failed: {e}")